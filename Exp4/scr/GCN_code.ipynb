{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing citeseer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing citeseer\n",
    "\n",
    "content_path = \"citeseer/citeseer.content\"\n",
    "cites_path = \"citeseer/citeseer.cites\"\n",
    "new_content_path = \"citeseer_new/citeseer.content.new\"\n",
    "new_cites_path = \"citeseer_new/citeseer.cites.new\"\n",
    "fr_content = open(content_path, \"r\")\n",
    "fr_cites = open(cites_path, \"r\")\n",
    "fw_content = open(new_content_path, \"w\")\n",
    "fw_cites = open(new_cites_path, \"w\")\n",
    "\n",
    "name_dict = dict()\n",
    "ban_set = set()\n",
    "content_lines = fr_content.readlines()\n",
    "cites_lines = fr_cites.readlines()\n",
    "\n",
    "import random\n",
    "\n",
    "for line in content_lines:\n",
    "    line = line.strip().split('\t')\n",
    "    name = line[0]\n",
    "    try:\n",
    "        x = int(name)\n",
    "        if x in ban_set:\n",
    "            print(x)\n",
    "        ban_set.add(x)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "for line in content_lines:\n",
    "    line = line.strip().split('\t')\n",
    "    name = line[0]\n",
    "    try:\n",
    "        int(name)\n",
    "        continue\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    encode = random.randint(0, 200000)\n",
    "    while encode in ban_set:\n",
    "        encode = random.randint(0, 200000)\n",
    "    ban_set.add(encode)\n",
    "    name_dict[name] = str(encode)\n",
    "\n",
    "for line in content_lines:\n",
    "    temp = line\n",
    "    line = line.strip().split('\t')\n",
    "    name = line[0]\n",
    "    try:\n",
    "        int(name)\n",
    "        fw_content.write(temp)\n",
    "    except ValueError:\n",
    "        temp = temp.replace(name, name_dict[name])\n",
    "        fw_content.write(temp)\n",
    "\n",
    "for line in cites_lines:\n",
    "    line = line.strip().split('\t')\n",
    "    n1 = line[0]\n",
    "    n2 = line[1]\n",
    "    try:\n",
    "        int(n1)\n",
    "        if n1 not in ban_set:\n",
    "            continue\n",
    "        n1 = str(n1)\n",
    "    except ValueError:\n",
    "        if n1 not in name_dict:\n",
    "            continue\n",
    "        n1 = name_dict[n1]\n",
    "    \n",
    "    try:\n",
    "        int(n2)\n",
    "        if n2 not in ban_set:\n",
    "            continue\n",
    "        n2 = str(n2)\n",
    "    except ValueError:\n",
    "        if n2 not in name_dict:\n",
    "            continue\n",
    "        n2 = name_dict[n2]\n",
    "\n",
    "    fw_cites.write(n1+'\t'+n2+'\\n')\n",
    " \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch_geometric.utils import negative_sampling\n",
    "import json\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(dataset, task, self_loop):\n",
    "    \n",
    "    if dataset == 'cora':\n",
    "        path = \"cora/\"\n",
    "        dataset = \"cora\"\n",
    "    elif dataset == 'citeseer':\n",
    "        #path = \"../datasets/citeseer_new/\"\n",
    "        path = \"citeseer_new/\"\n",
    "        dataset = \"citeseer\"\n",
    "    \n",
    "    #print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    np.random.shuffle(idx_features_labels)\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    \n",
    "    temp1 = map(idx_map.get, edges_unordered.flatten())\n",
    "    temp2 = list(temp1)\n",
    "    x = list(edges_unordered.flatten())\n",
    "    print(x[462])\n",
    "    for i in range(len(temp2)):\n",
    "        elem = temp2[i]\n",
    "        try:\n",
    "            elem = int(elem)\n",
    "        except TypeError:\n",
    "            print(i)\n",
    "\n",
    "    edges = np.array(temp2, dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    '''\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    '''\n",
    "    #print('You are currently running {} task on {} dataset...'.format(task, dataset))\n",
    "    if task == 'linkpred':\n",
    "        edge_num = edges.shape[0]\n",
    "        shuffled_ids = np.random.permutation(edge_num)\n",
    "        test_set_size = int(edge_num * 0.15)\n",
    "        val_set_size = int(edge_num * 0.15)\n",
    "        test_ids = shuffled_ids[ : test_set_size]\n",
    "        val_ids = shuffled_ids[test_set_size : test_set_size + val_set_size]\n",
    "        train_ids = shuffled_ids[test_set_size + val_set_size : ]\n",
    "\n",
    "        train_pos_edges = torch.tensor(edges[train_ids], dtype=int)\n",
    "        val_pos_edges = torch.tensor(edges[val_ids], dtype=int)\n",
    "        test_pos_edges = torch.tensor(edges[test_ids], dtype=int)\n",
    "\n",
    "        train_pos_edges = torch.transpose(train_pos_edges, 1, 0)\n",
    "        # shape = [2, train_pos_edge_num]\n",
    "        val_pos_edges = torch.transpose(val_pos_edges, 1, 0)\n",
    "        test_pos_edges = torch.transpose(test_pos_edges, 1, 0)\n",
    "\n",
    "        def negative_sample(pos_edges, nodes_num):\n",
    "            '''\n",
    "            pos_edges = [[src_1,...],\n",
    "                        [dst_1,...]]\n",
    "            '''\n",
    "            neg_edges = negative_sampling(\n",
    "                edge_index=pos_edges,\n",
    "                num_nodes=nodes_num,\n",
    "                num_neg_samples=pos_edges.shape[1],\n",
    "                method='sparse'\n",
    "            )\n",
    "            edges = torch.cat((pos_edges, neg_edges), dim=-1)\n",
    "            '''\n",
    "            edges = [[src_1,src_2,...,src_m],\n",
    "                    [dst_1,dst_2,...,dst_m]]\n",
    "            shape = [2, 2*train_edge_num]\n",
    "            '''\n",
    "            edges_label = torch.cat((\n",
    "                torch.ones(pos_edges.shape[1]),\n",
    "                torch.zeros(neg_edges.shape[1])\n",
    "            ),dim=0)\n",
    "            # size = [2*train_edge_num]\n",
    "            return edges, edges_label\n",
    "        \n",
    "        train_edges, train_label = negative_sample(train_pos_edges, idx.shape[0])\n",
    "        val_edges, val_label = negative_sample(val_pos_edges, idx.shape[0])\n",
    "        test_edges, test_label = negative_sample(test_pos_edges, idx.shape[0])\n",
    "        \n",
    "        adj = sp.coo_matrix((np.ones(train_pos_edges.shape[1]), (train_pos_edges[0], train_pos_edges[1])),\n",
    "                        shape=(idx.shape[0], idx.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "        # build symmetric adjacency matrix\n",
    "        adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "        features = normalize(features)\n",
    "    \n",
    "        if self_loop == True:\n",
    "            adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "        else:\n",
    "            adj = normalize(adj)\n",
    "\n",
    "        features = torch.FloatTensor(np.array(features.todense()))\n",
    "        adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "        train_edges = train_edges.tolist()\n",
    "        val_edges = val_edges.tolist()\n",
    "        test_edges = test_edges.tolist()\n",
    "        train_label = train_label.type(torch.float)\n",
    "        val_label = val_label.type(torch.float)\n",
    "        test_label = test_label.type(torch.float)\n",
    "\n",
    "        return adj, features, train_edges, val_edges, test_edges, \\\n",
    "                    train_label, val_label, test_label\n",
    "\n",
    "    elif task == 'nodecls':\n",
    "        adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "        # build symmetric adjacency matrix\n",
    "        adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "        features = normalize(features)\n",
    "    \n",
    "        if self_loop == True:\n",
    "            adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "        else:\n",
    "            adj = normalize(adj)\n",
    "        \n",
    "        # split train || val || test\n",
    "        idx_train = range(140)\n",
    "        idx_val = range(200, 500)\n",
    "        idx_test = range(500, 1500)\n",
    "        \n",
    "        features = torch.FloatTensor(np.array(features.todense()))\n",
    "        labels = torch.LongTensor(np.where(labels)[1])\n",
    "        adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "        idx_train = torch.LongTensor(idx_train)\n",
    "        idx_val = torch.LongTensor(idx_val)\n",
    "        idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "        return adj, features, labels, idx_train, idx_val, idx_test\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"hyper-parameter `task` belongs to \\{'nodecls', 'linkpred'\\}.\")\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "        \n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "def load_ppi_data(task='nodecls', self_loop=True):\n",
    "    path = \"ppi/\"\n",
    "    #print('Loading PPI dataset...')\n",
    "    feature_file = path + \"ppi-feats.npy\"\n",
    "    label_file = path + \"ppi-class_map.json\"\n",
    "    edge_file = path + \"ppi-walks.txt\"\n",
    "    graph_file = path + \"ppi-G.json\"\n",
    "\n",
    "    #print('Uploading features ...')\n",
    "    features = np.load(feature_file) # shape = (56944, 50)\n",
    "    features = sp.csr_matrix(features, dtype=np.float32)\n",
    "    #print('Uploading labels...')\n",
    "    fr_label = open(label_file, \"r\")\n",
    "    label_dict = json.load(fr_label)\n",
    "    proc_label_dict = dict()\n",
    "    for key in label_dict:\n",
    "        proc_label_dict[int(key)] = list(label_dict[key])\n",
    "    _labels = sorted(proc_label_dict.items(), key=lambda d: d[0])\n",
    "    labels = list()\n",
    "    for item in _labels:\n",
    "        _, x = item\n",
    "        labels.append(x)\n",
    "    labels = np.array(labels, dtype=np.int32)\n",
    "    print('Uploading graph...')\n",
    "    fr_graph = open(graph_file, \"r\")\n",
    "    graph_dict = json.load(fr_graph)\n",
    "    nodes = graph_dict[\"nodes\"]\n",
    "    links = graph_dict[\"links\"]\n",
    "    #print('Generating edges')\n",
    "    edges = [[links[i][\"source\"], links[i][\"target\"]] for i in range(len(links))]\n",
    "    edges = np.array(edges, dtype=np.int32)\n",
    "    #print('Generating nodes')\n",
    "    idx = list()\n",
    "    idx_train = list()\n",
    "    idx_val = list()\n",
    "    idx_test = list()\n",
    "    for i in range(len(nodes)):\n",
    "        idx.append(nodes[i][\"id\"])\n",
    "        if nodes[i][\"test\"] == True:\n",
    "            idx_test.append(nodes[i][\"id\"])\n",
    "        elif nodes[i][\"val\"] == True:\n",
    "            idx_val.append(nodes[i][\"id\"])\n",
    "        else:\n",
    "            idx_train.append(nodes[i][\"id\"])\n",
    "    idx = np.array(idx, dtype=np.int32)\n",
    "\n",
    "    #print('You are currently running {} task on PPI dataset...'.format(task))\n",
    "    if task == 'linkpred':\n",
    "        edge_num = edges.shape[0]\n",
    "        shuffled_ids = np.random.permutation(edge_num)\n",
    "        test_set_size = int(edge_num * 0.15)\n",
    "        val_set_size = int(edge_num * 0.15)\n",
    "        test_ids = shuffled_ids[ : test_set_size]\n",
    "        val_ids = shuffled_ids[test_set_size : test_set_size + val_set_size]\n",
    "        train_ids = shuffled_ids[test_set_size + val_set_size : ]\n",
    "\n",
    "        train_pos_edges = torch.tensor(edges[train_ids], dtype=int)\n",
    "        val_pos_edges = torch.tensor(edges[val_ids], dtype=int)\n",
    "        test_pos_edges = torch.tensor(edges[test_ids], dtype=int)\n",
    "\n",
    "        train_pos_edges = torch.transpose(train_pos_edges, 1, 0)\n",
    "        # shape = [2, train_pos_edge_num]\n",
    "        val_pos_edges = torch.transpose(val_pos_edges, 1, 0)\n",
    "        test_pos_edges = torch.transpose(test_pos_edges, 1, 0)\n",
    "\n",
    "        def negative_sample(pos_edges, nodes_num):\n",
    "            '''\n",
    "            pos_edges = [[src_1,...],\n",
    "                        [dst_1,...]]\n",
    "            '''\n",
    "            neg_edges = negative_sampling(\n",
    "                edge_index=pos_edges,\n",
    "                num_nodes=nodes_num,\n",
    "                num_neg_samples=pos_edges.shape[1],\n",
    "                method='sparse'\n",
    "            )\n",
    "            edges = torch.cat((pos_edges, neg_edges), dim=-1)\n",
    "            '''\n",
    "            edges = [[src_1,src_2,...,src_m],\n",
    "                    [dst_1,dst_2,...,dst_m]]\n",
    "            shape = [2, 2*train_edge_num]\n",
    "            '''\n",
    "            edges_label = torch.cat((\n",
    "                torch.ones(pos_edges.shape[1]),\n",
    "                torch.zeros(neg_edges.shape[1])\n",
    "            ),dim=0)\n",
    "            # size = [2*train_edge_num]\n",
    "            return edges, edges_label\n",
    "        \n",
    "        train_edges, train_label = negative_sample(train_pos_edges, idx.shape[0])\n",
    "        val_edges, val_label = negative_sample(val_pos_edges, idx.shape[0])\n",
    "        test_edges, test_label = negative_sample(test_pos_edges, idx.shape[0])\n",
    "        \n",
    "        adj = sp.coo_matrix((np.ones(train_pos_edges.shape[1]), (train_pos_edges[0], train_pos_edges[1])),\n",
    "                        shape=(idx.shape[0], idx.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "        # build symmetric adjacency matrix\n",
    "        adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "        features = normalize(features)\n",
    "    \n",
    "        if self_loop == True:\n",
    "            adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "        else:\n",
    "            adj = normalize(adj)\n",
    "\n",
    "        features = torch.FloatTensor(np.array(features.todense()))\n",
    "        adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "        train_edges = train_edges.tolist()\n",
    "        val_edges = val_edges.tolist()\n",
    "        test_edges = test_edges.tolist()\n",
    "        train_label = train_label.type(torch.float)\n",
    "        val_label = val_label.type(torch.float)\n",
    "        test_label = test_label.type(torch.float)\n",
    "\n",
    "        return adj, features, train_edges, val_edges, test_edges, \\\n",
    "                    train_label, val_label, test_label\n",
    "\n",
    "    elif task == 'nodecls':\n",
    "        adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "        # build symmetric adjacency matrix\n",
    "        adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "        features = normalize(features)\n",
    "    \n",
    "        if self_loop == True:\n",
    "            adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "        else:\n",
    "            adj = normalize(adj)\n",
    "        \n",
    "        \n",
    "        features = torch.FloatTensor(np.array(features.todense()))\n",
    "        labels = torch.LongTensor(labels)\n",
    "        adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "        idx_train = torch.LongTensor(idx_train)\n",
    "        idx_val = torch.LongTensor(idx_val)\n",
    "        idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "        return adj, features, labels, idx_train, idx_val, idx_test\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"hyper-parameter `task` belongs to \\{'nodecls', 'linkpred'\\}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layers\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from layers import GraphConvolution\n",
    "from torch_geometric.nn import PairNorm\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hid_channels, out_channels, dropout, \n",
    "                 layer_num=2, activation='relu', drop_edge=False, pair_norm=False):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc_inp = GraphConvolution(in_channels, hid_channels)\n",
    "        self.gc_hids = nn.ModuleList([GraphConvolution(hid_channels, hid_channels) for _ in range(layer_num-2)])\n",
    "        self.gc_out = GraphConvolution(hid_channels, out_channels)\n",
    "        if activation == 'relu':\n",
    "            self.activate = F.relu\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activate = torch.sigmoid\n",
    "        elif activation == 'tanh':\n",
    "            self.activate =  torch.tanh\n",
    "\n",
    "        \n",
    "        self.pair_norm = pair_norm\n",
    "        if pair_norm == True:\n",
    "            self.norm = PairNorm()\n",
    "        \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # for ppi dataset\n",
    "        self.linear_out = nn.Linear(out_channels, 121)\n",
    "\n",
    "    def forward(self, x, adj, task='nodecls', edges=None, ppi=False):\n",
    "        x = self.gc_inp(x, adj)\n",
    "        x = self.activate(x)\n",
    "\n",
    "        for gc_layer in self.gc_hids:\n",
    "            x = self.dropout(x)\n",
    "            x = gc_layer(x, adj)\n",
    "            \n",
    "            if self.pair_norm:\n",
    "                x = self.norm(x)\n",
    "            \n",
    "            x = self.activate(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.gc_out(x, adj)\n",
    "\n",
    "        if task == 'nodecls':\n",
    "            if ppi == False:\n",
    "                # x.shape = [node_num, label_class_num]\n",
    "                return F.log_softmax(x, dim=1)\n",
    "            else:\n",
    "                x = self.linear_out(x)\n",
    "                # x.shape = [node_num, label_dim]\n",
    "                return x\n",
    "        elif task == 'linkpred':\n",
    "            # x.shape = [node_num, hid_channels]\n",
    "            assert edges != None\n",
    "            src = x[edges[0]] # shape = [src_num, hid_channels]\n",
    "            dst = x[edges[1]] # shape = [node_num, hid_channels]\n",
    "            inner_prods = (src * dst).sum(dim=-1) # shape =[src_num]\n",
    "            return inner_prods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "import matplotlib\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "# Replace the following imports with your actual implementations\n",
    "# from utils import load_data, accuracy, load_ppi_data\n",
    "# from models import GCN\n",
    "\n",
    "# Set your desired values for the arguments\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.no_cuda = False\n",
    "        self.fastmode = False\n",
    "        self.seed = 42\n",
    "        self.epochs = 50\n",
    "        self.lr = 0.01\n",
    "        self.weight_decay = 5e-4\n",
    "        self.hidden = 16\n",
    "        self.dropout = 0.01\n",
    "        self.drop_edge = 0.0   #Exp 2\n",
    "        self.pair_norm = True  #Exp 2\n",
    "        self.self_loop = True  #Exp 1\n",
    "        self.layer_num = 3  #Exp 1\n",
    "        self.activate = 'relu'  #Exp 3\n",
    "        self.dataset = 'citeseer'  # Set your dataset\n",
    "        self.task = 'nodecls'  # Set your task\n",
    "\n",
    "args = Args()\n",
    "\n",
    "print(args.self_loop)\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Load data\n",
    "if args.task == 'nodecls':\n",
    "    if args.dataset == 'cora' or args.dataset == 'citeseer':\n",
    "        adj, features, labels, idx_train, idx_val, idx_test = load_data(\n",
    "            dataset=args.dataset,\n",
    "            task=args.task,\n",
    "            self_loop=args.self_loop\n",
    "        )\n",
    "        # Model and optimizer\n",
    "        model = GCN(in_channels=features.shape[1],\n",
    "                    hid_channels=args.hidden,\n",
    "                    out_channels=labels.max().item() + 1,\n",
    "                    dropout=args.dropout,\n",
    "                    layer_num=args.layer_num,\n",
    "                    activation=args.activate,\n",
    "                    drop_edge=args.drop_edge,\n",
    "                    pair_norm=args.pair_norm)\n",
    "    elif args.dataset == 'ppi':\n",
    "        adj, features, labels, idx_train, idx_val, idx_test = load_ppi_data(\n",
    "            task=args.task,\n",
    "            self_loop=args.self_loop\n",
    "        )\n",
    "        # Model and optimizer\n",
    "        model = GCN(in_channels=features.shape[1],\n",
    "                    hid_channels=args.hidden,\n",
    "                    out_channels=args.hidden,\n",
    "                    dropout=args.dropout,\n",
    "                    layer_num=args.layer_num,\n",
    "                    activation=args.activate,\n",
    "                    drop_edge=args.drop_edge,\n",
    "                    pair_norm=args.pair_norm)\n",
    "\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "        features = features.cuda()\n",
    "        adj = adj.cuda()\n",
    "        labels = labels.cuda()\n",
    "        idx_train = idx_train.cuda()\n",
    "        idx_val = idx_val.cuda()\n",
    "        idx_test = idx_test.cuda()\n",
    "\n",
    "elif args.task == 'linkpred':\n",
    "    if args.dataset == 'cora' or args.dataset == 'citeseer':\n",
    "        adj, features, train_edges, val_edges, test_edges, \\\n",
    "        train_label, val_label, test_label = load_data(\n",
    "            dataset=args.dataset,\n",
    "            task=args.task,\n",
    "            self_loop=args.self_loop\n",
    "        )\n",
    "    elif args.dataset == 'ppi':\n",
    "        adj, features, train_edges, val_edges, test_edges, \\\n",
    "        train_label, val_label, test_label = load_ppi_data(\n",
    "            task=args.task,\n",
    "            self_loop=args.self_loop\n",
    "        )\n",
    "    model = GCN(in_channels=features.shape[1],\n",
    "                hid_channels=args.hidden,\n",
    "                out_channels=args.hidden,\n",
    "                dropout=args.dropout,\n",
    "                layer_num=args.layer_num,\n",
    "                activation=args.activate,\n",
    "                drop_edge=args.drop_edge,\n",
    "                pair_norm=args.pair_norm)\n",
    "\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "        features = features.cuda()\n",
    "        adj = adj.cuda()\n",
    "        train_label = train_label.cuda()\n",
    "        val_label = val_label.cuda()\n",
    "        test_label = test_label.cuda()\n",
    "\n",
    "else:\n",
    "    raise Exception('task({}) is supposed to belong to {\"nodecls\", \"linkpred\"}.'.format(task))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "if args.task == 'nodecls':\n",
    "    if args.dataset != 'ppi':\n",
    "        criterion = F.nll_loss\n",
    "    else:\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "elif args.task == 'linkpred':\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "val_performances = list()\n",
    "test_performances = list()\n",
    "train_losses = []  # to store training loss for plotting\n",
    "\n",
    "def train(epoch, task='nodecls'):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if task == 'nodecls':\n",
    "        if args.dataset != 'ppi':\n",
    "            output = model(features, adj)\n",
    "            loss_train = criterion(output[idx_train], labels[idx_train])\n",
    "        else:\n",
    "            output = model(x=features, adj=adj, ppi=True)\n",
    "            loss_train = criterion(output[idx_train], labels[idx_train].float())\n",
    "\n",
    "        if args.dataset != 'ppi':\n",
    "            acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "        else:\n",
    "            preds = (output[idx_train] > 0).float().cpu()\n",
    "            f1_train = f1_score(labels[idx_train].cpu(), preds, average='micro')\n",
    "\n",
    "    elif task == 'linkpred':\n",
    "        output = model(features, adj, 'linkpred', train_edges)\n",
    "        loss_train = criterion(output, train_label)\n",
    "        logits = torch.sigmoid(output)\n",
    "        auc_train = roc_auc_score(train_label.cpu().numpy(), logits.detach().cpu().numpy())\n",
    "\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_losses.append(loss_train.item())  # append the current training loss\n",
    "\n",
    "    model.eval()\n",
    "    if task == 'nodecls':\n",
    "        if args.dataset != 'ppi':\n",
    "            output = model(features, adj)\n",
    "            loss_val = criterion(output[idx_val], labels[idx_val])\n",
    "        else:\n",
    "            output = model(x=features, adj=adj, ppi=True)\n",
    "            loss_val = criterion(output[idx_val], labels[idx_val].float())\n",
    "\n",
    "        if args.dataset != 'ppi':\n",
    "            acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "        else:\n",
    "            preds = (output[idx_val] > 0).float().cpu()\n",
    "            f1_val = f1_score(labels[idx_val].cpu(), preds, average='micro')\n",
    "\n",
    "        if args.dataset != 'ppi':\n",
    "            loss_test = criterion(output[idx_test], labels[idx_test])\n",
    "            acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "        else:\n",
    "            loss_test = criterion(output[idx_test], labels[idx_test].float())\n",
    "            preds = (output[idx_test] > 0).float().cpu()\n",
    "            f1_test = f1_score(labels[idx_test].cpu(), preds, average='micro')\n",
    "\n",
    "        if args.dataset != 'ppi':\n",
    "            val_performances.append(acc_val.item())\n",
    "            test_performances.append(acc_test.item())\n",
    "        else:\n",
    "            val_performances.append(f1_val.item())\n",
    "            test_performances.append(f1_test.item())\n",
    "\n",
    "    elif task == 'linkpred':\n",
    "        output = model(features, adj, 'linkpred', val_edges)\n",
    "        loss_val = criterion(output, val_label)\n",
    "        logits = torch.sigmoid(output)\n",
    "        auc_val = roc_auc_score(val_label.cpu().numpy(), logits.detach().cpu().numpy())\n",
    "\n",
    "        output = model(features, adj, 'linkpred', test_edges)\n",
    "        loss_test = criterion(output, test_label)\n",
    "        logits = torch.sigmoid(output)\n",
    "        auc_test = roc_auc_score(test_label.cpu().numpy(), logits.detach().cpu().numpy())\n",
    "\n",
    "        val_performances.append(auc_val)\n",
    "        test_performances.append(auc_test)\n",
    "\n",
    "def test(task='nodecls'):\n",
    "    if task == 'nodecls':\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "        loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "        acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    elif task == 'linkpred':\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(features, adj, 'linkpred', test_edges)\n",
    "            loss_test = criterion(output, test_label)\n",
    "            logits = torch.sigmoid(output)\n",
    "        auc_test = roc_auc_score(test_label.cpu().numpy(), logits.detach().cpu().numpy())\n",
    "\n",
    "def output_best(val_performances, test_performances, task='nodecls'):\n",
    "    val_performances = np.array(val_performances)\n",
    "    max_id = np.argmax(val_performances)\n",
    "    if task == 'linkpred':\n",
    "        print(\"Test set results (with best validation performance):\",\n",
    "              \"auc score= {:.4f}\".format(test_performances[max_id]))\n",
    "        pass\n",
    "    else:\n",
    "        if args.dataset != 'ppi':\n",
    "            print(\"Test set results (with best validation performance):\",\n",
    "                  \"acc = {:.4f}\".format(test_performances[max_id]))\n",
    "        else:\n",
    "            print(\"Test set results (with best validation performance):\",\n",
    "                  \"f1_score = {:.4f}\".format(test_performances[max_id]))\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args.task)\n",
    "\n",
    "# Testing\n",
    "# test(args.task)\n",
    "print('dataset:', args.dataset, ' --- task:', args.task,\n",
    "      ' --- self_loop:', args.self_loop,\n",
    "      ' --- layer_num:', args.layer_num,\n",
    "      ' --- pair_norm:', args.pair_norm,\n",
    "      ' --- activate:', args.activate,\n",
    "      ' --- hidden:', args.hidden)\n",
    "\n",
    "\n",
    "output_best(val_performances, test_performances, args.task)\n",
    "\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "print('-----------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "# from utils import load_data, load_ppi_data\n",
    "# from models import GCN\n",
    "\n",
    "# Set your desired values for the arguments\n",
    "class TestArgs:\n",
    "    def __init__(self):\n",
    "        self.no_cuda = False\n",
    "        self.fastmode = False\n",
    "        self.seed = 42\n",
    "        self.epochs = 50 # Set a smaller number of epochs for testing\n",
    "        self.lr = 0.01\n",
    "        self.weight_decay = 5e-4\n",
    "        self.hidden = 16\n",
    "        self.dropout = 0.5\n",
    "        self.drop_edge = 0.0  #Exp 2\n",
    "        self.pair_norm = True  #Exp 2  \n",
    "        self.self_loop = True  #Exp 1\n",
    "        self.layer_num = 3  #Exp 1\n",
    "        self.activate = 'relu'  #Exp 3\n",
    "        self.dataset = 'cora'  # Choose the dataset for testing\n",
    "        self.task = 'linkpred'  # Choose the task for testing ('nodecls' or 'linkpred')\n",
    "\n",
    "test_args = TestArgs()\n",
    "\n",
    "# Load data for testing\n",
    "if test_args.task == 'nodecls':\n",
    "    adj, features, labels, idx_train, idx_val, idx_test = load_data(\n",
    "        dataset=test_args.dataset,\n",
    "        task=test_args.task,\n",
    "        self_loop=test_args.self_loop\n",
    "    )\n",
    "elif test_args.task == 'linkpred':\n",
    "    adj, features, train_edges, val_edges, test_edges, \\\n",
    "    train_label, val_label, test_label = load_data(\n",
    "        dataset=test_args.dataset,\n",
    "        task=test_args.task,\n",
    "        self_loop=test_args.self_loop\n",
    "    )\n",
    "\n",
    "# Model and optimizer for testing\n",
    "model = GCN(in_channels=features.shape[1],\n",
    "            hid_channels=test_args.hidden,\n",
    "            out_channels=labels.max().item() + 1,\n",
    "            dropout=test_args.dropout,\n",
    "            layer_num=test_args.layer_num,\n",
    "            activation=test_args.activate,\n",
    "            drop_edge=test_args.drop_edge,\n",
    "            pair_norm=test_args.pair_norm)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=test_args.lr, weight_decay=test_args.weight_decay)\n",
    "\n",
    "# Test the model\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "if test_args.task == 'nodecls':\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "else:\n",
    "    output = model(features, adj, 'linkpred', train_edges)\n",
    "    loss_train = torch.nn.BCEWithLogitsLoss()(output, train_label)\n",
    "\n",
    "loss_train.backward()\n",
    "optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on validation set for parameter tuning\n",
    "if test_args.task == 'nodecls':\n",
    "    output_val = model(features, adj)\n",
    "    loss_val = F.nll_loss(output_val[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy_score(labels[idx_val].cpu().numpy(), output_val[idx_val].argmax(dim=1).cpu().numpy())\n",
    "    print(f'Validation Accuracy: {acc_val:.4f}')\n",
    "else:\n",
    "    output_val = model(features, adj, 'linkpred', val_edges)\n",
    "    loss_val = torch.nn.BCEWithLogitsLoss()(output_val, val_label)\n",
    "    auc_val = roc_auc_score(val_label.cpu().numpy(), torch.sigmoid(output_val).detach().cpu().numpy())\n",
    "    print(f'Validation AUC: {auc_val:.4f}')\n",
    "\n",
    "# Test the model on the test set\n",
    "if test_args.task == 'nodecls':\n",
    "    output_test = model(features, adj)\n",
    "    loss_test = F.nll_loss(output_test[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy_score(labels[idx_test].cpu().numpy(), output_test[idx_test].argmax(dim=1).cpu().numpy())\n",
    "    print(f'Test Accuracy: {acc_test:.4f}')\n",
    "else:\n",
    "    output_test = model(features, adj, 'linkpred', test_edges)\n",
    "    loss_test = torch.nn.BCEWithLogitsLoss()(output_test, test_label)\n",
    "    auc_test = roc_auc_score(test_label.cpu().numpy(), torch.sigmoid(output_test).detach().cpu().numpy())\n",
    "    print(f'Test AUC: {auc_test:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Top 1 Accuracy over epochs for validation and test sets\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(val_performances, label='Validation', marker='o')\n",
    "plt.plot(test_performances, label='Test', marker='o')\n",
    "plt.title('Top 1 Accuracy over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Top 1 Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the training loss over epochs\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_losses, label='Training Loss', marker='o', color='orange')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
